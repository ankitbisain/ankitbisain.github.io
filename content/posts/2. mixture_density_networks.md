title: Fitting Multimodal Distributions with Mixture Density Networks
date: 2024-04-13 10:25
category: machine learning
tags: machine learning, classification, tutorial
slug: mixture_density_networks
summary: Mixture Density networks are cool.

In regression problems, we often model the conditional mean of a conditional probability distribution $p(\textbf{t}|\textbf{x})$ by minimizing the
least squares (L2-loss) between the input and target variables (this can be shown through a brief derivation using the calculus of variations).
For more complicated multimodal distributions, however, using merely the conditional mean of a univariate Gaussian can prove to be problematic. 

In this blog post, we show how a neural network can represent more general distributions (in our case, we fit a Gaussian mixture model) and then 
show how one might implement such a network in code. Such models are known as *mixture density networks*, and we will see how to model the errors
and corresponding outputs. 

## Motivation

For simple regression problems, we can model the conditional distribution $p(\textbf{t}|\textbf{x})$ using a Gaussian. This can leaead to disastrous
results however, for outputs with non-Gaussian distributions. This can be seen, for example, in *inverse problems*. For a simple example, consider 
the example of the inverse problem in *robot kinematics* shown below: 

<figure>
<img src="/images/2_robot_arm.png" alt = "An example of a multimodal distribuution using robot kinematics"/>
<figcaption> <i>Left:</i> A two-link robot arm with the coordinates $(x_1,x_2)$ of the end-effector deermined by the parameters $\theta_1,\theta_2,L_1,L_2$. 
<i>Right:</i> The <i>inverse</i> problem involves finding a set of parameters $\theta_1,\theta_2,L_1,L_2$ such that an end-effector position of $(x_1,x_2)$ 
is achieved. Note we have two solutions corresponding to an "elbow up" and an "elbow down" position showing that the problem is intrinsically multimodal.
</figure>

The *forward problem* involves finding the end-effector position of a two-link robot arm that has arm lengths $L_1$ and $L_2$, 
is tilted at an angle $\theta_1$ from the ground, and has an inter-arm angle of $\theta_2$. The forward problem, given the set of
parameters, has a unique solution. However, the *inverse problem* of finding the corresponding angles $\theta_j$ and lengths $L_j$ 
has multiple solutions (and thus multiple modes) as shown in the second image. Similarly, other inverse problems are readily seen 
to be multimodal.

For example, consider a data set $\mathcal{D}$ of elements in $\mathbb{R}^2$ generated as follows: sample a set of values $\{x_n\}$ uniformly from the interval
$(0,1)$ and define the corresponding target values $t_n := x_n+0.3\sin(2\pi{x}) + r(-0.1,0.1)$ where $r(-0.1,0.1)$ is some uniform noise in the range $-0.1$
to $0.1$. 

The forward regression problem of mapping a function $x -> t$ can readily be solved by doing maximum-likelihood under a Gaussian distribution 
using the standard L2-loss (say, by fitting a neural network or defining suitable basis functions and then doing linear regression). The inverse problem, 
however, of finding a mapping $t -> x$ leads to very poor results. We show this by fitting a two-layer neural network to both problems and minimizing the L2-loss:

```python

```
